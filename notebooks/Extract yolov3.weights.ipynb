{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/qqwweee/keras-yolo3\n",
    "    - `convert.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Config\" data-toc-modified-id=\"Config-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Config</a></span><ul class=\"toc-item\"><li><span><a href=\"#Path\" data-toc-modified-id=\"Path-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Path</a></span></li></ul></li><li><span><a href=\"#yolov3.weight\" data-toc-modified-id=\"yolov3.weight-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>yolov3.weight</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:05:55.548936Z",
     "start_time": "2019-02-14T07:05:55.544061Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import pathlib\n",
    "import collections\n",
    "import io\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T06:54:54.576831Z",
     "start_time": "2019-02-14T06:54:54.571240Z"
    }
   },
   "outputs": [],
   "source": [
    "HOME_Path = pathlib.Path(os.getcwd())\n",
    "\n",
    "config_Path = HOME_Path / \"yolov3.cfg\"\n",
    "assert config_Path.exists\n",
    "assert config_Path.name.endswith(\".cfg\"), \"{} is not a .cfg file\".format(config_Path)\n",
    "\n",
    "weights_Path = HOME_Path / \"yolov3.weights\"\n",
    "assert weights_Path.exists\n",
    "assert weights_Path.name.endswith(\".weights\"), \"{} is not a .weights file\".format(weights_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yolov3.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T06:59:03.961625Z",
     "start_time": "2019-02-14T06:59:03.957678Z"
    }
   },
   "outputs": [],
   "source": [
    "weights_file = open(file=str(weights_Path), mode=\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:05:57.248033Z",
     "start_time": "2019-02-14T07:05:57.239422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Header\n",
      "major    : 0\n",
      "minor    : 2\n",
      "revision : 0\n",
      "seen     : [32013312]\n"
     ]
    }
   ],
   "source": [
    "major, minor, revision = np.ndarray(shape=(3, ), dtype='int32', buffer=weights_file.read(12))\n",
    "\n",
    "if (major*10+minor)>=2 and major<1000 and minor<1000:\n",
    "    seen = np.ndarray(shape=(1,), dtype='int64', buffer=weights_file.read(8))\n",
    "else:\n",
    "    seen = np.ndarray(shape=(1,), dtype='int32', buffer=weights_file.read(4))\n",
    "\n",
    "print(\"Weights Header\\n\" +\n",
    "      \"major    : {}\\n\".format(major) +\n",
    "      \"minor    : {}\\n\".format(minor) +\n",
    "      \"revision : {}\\n\".format(revision) +\n",
    "      \"seen     : {}\".format(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:05:57.371230Z",
     "start_time": "2019-02-14T07:05:57.365688Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique_config_sections(config_file):\n",
    "    \"\"\"\n",
    "    Convert all config sections to have unique names.\n",
    "    Adds unique suffixes to config sections for compability with configparser.\n",
    "    \"\"\"\n",
    "    # [defaultdict の 利用 - Python defaultdict の使い方 - Qiita](https://qiita.com/xza/items/72a1b07fcf64d1f4bdb7#defaultdict-%E3%81%AE-%E5%88%A9%E7%94%A8)\n",
    "    section_counters = collections.defaultdict(int)  # defaultdictは関数でdictのvalueを初期化\n",
    "    output_stream = io.StringIO()\n",
    "    with open(config_file) as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('['):\n",
    "                section = line.strip().strip('[]')\n",
    "                _section = section + '_' + str(section_counters[section])\n",
    "                section_counters[section] += 1\n",
    "                line = line.replace(section, _section)\n",
    "            output_stream.write(line)\n",
    "    output_stream.seek(0)\n",
    "    return output_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:05:58.200759Z",
     "start_time": "2019-02-14T07:05:58.183589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Darknet config.\n"
     ]
    }
   ],
   "source": [
    "print('Parsing Darknet config.')\n",
    "unique_config_file = unique_config_sections(config_file=str(config_Path))\n",
    "\n",
    "cfg_parser = configparser.ConfigParser()\n",
    "cfg_parser.read_file(f=unique_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:15:39.137708Z",
     "start_time": "2019-02-14T07:15:39.133746Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_decay = float(cfg_parser['net_0']['decay']) if 'net_0' in cfg_parser.sections() else 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:16:17.182393Z",
     "start_time": "2019-02-14T07:16:17.173344Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing section net_0\n",
      "Parsing section convolutional_0\n",
      "Parsing section convolutional_1\n",
      "Parsing section convolutional_2\n",
      "Parsing section convolutional_3\n",
      "Parsing section shortcut_0\n",
      "Parsing section convolutional_4\n",
      "Parsing section convolutional_5\n",
      "Parsing section convolutional_6\n",
      "Parsing section shortcut_1\n",
      "Parsing section convolutional_7\n",
      "Parsing section convolutional_8\n",
      "Parsing section shortcut_2\n",
      "Parsing section convolutional_9\n",
      "Parsing section convolutional_10\n",
      "Parsing section convolutional_11\n",
      "Parsing section shortcut_3\n",
      "Parsing section convolutional_12\n",
      "Parsing section convolutional_13\n",
      "Parsing section shortcut_4\n",
      "Parsing section convolutional_14\n",
      "Parsing section convolutional_15\n",
      "Parsing section shortcut_5\n",
      "Parsing section convolutional_16\n",
      "Parsing section convolutional_17\n",
      "Parsing section shortcut_6\n",
      "Parsing section convolutional_18\n",
      "Parsing section convolutional_19\n",
      "Parsing section shortcut_7\n",
      "Parsing section convolutional_20\n",
      "Parsing section convolutional_21\n",
      "Parsing section shortcut_8\n",
      "Parsing section convolutional_22\n",
      "Parsing section convolutional_23\n",
      "Parsing section shortcut_9\n",
      "Parsing section convolutional_24\n",
      "Parsing section convolutional_25\n",
      "Parsing section shortcut_10\n",
      "Parsing section convolutional_26\n",
      "Parsing section convolutional_27\n",
      "Parsing section convolutional_28\n",
      "Parsing section shortcut_11\n",
      "Parsing section convolutional_29\n",
      "Parsing section convolutional_30\n",
      "Parsing section shortcut_12\n",
      "Parsing section convolutional_31\n",
      "Parsing section convolutional_32\n",
      "Parsing section shortcut_13\n",
      "Parsing section convolutional_33\n",
      "Parsing section convolutional_34\n",
      "Parsing section shortcut_14\n",
      "Parsing section convolutional_35\n",
      "Parsing section convolutional_36\n",
      "Parsing section shortcut_15\n",
      "Parsing section convolutional_37\n",
      "Parsing section convolutional_38\n",
      "Parsing section shortcut_16\n",
      "Parsing section convolutional_39\n",
      "Parsing section convolutional_40\n",
      "Parsing section shortcut_17\n",
      "Parsing section convolutional_41\n",
      "Parsing section convolutional_42\n",
      "Parsing section shortcut_18\n",
      "Parsing section convolutional_43\n",
      "Parsing section convolutional_44\n",
      "Parsing section convolutional_45\n",
      "Parsing section shortcut_19\n",
      "Parsing section convolutional_46\n",
      "Parsing section convolutional_47\n",
      "Parsing section shortcut_20\n",
      "Parsing section convolutional_48\n",
      "Parsing section convolutional_49\n",
      "Parsing section shortcut_21\n",
      "Parsing section convolutional_50\n",
      "Parsing section convolutional_51\n",
      "Parsing section shortcut_22\n",
      "Parsing section convolutional_52\n",
      "Parsing section convolutional_53\n",
      "Parsing section convolutional_54\n",
      "Parsing section convolutional_55\n",
      "Parsing section convolutional_56\n",
      "Parsing section convolutional_57\n",
      "Parsing section convolutional_58\n",
      "Parsing section yolo_0\n",
      "Parsing section route_0\n",
      "Parsing section convolutional_59\n",
      "Parsing section upsample_0\n",
      "Parsing section route_1\n",
      "Parsing section convolutional_60\n",
      "Parsing section convolutional_61\n",
      "Parsing section convolutional_62\n",
      "Parsing section convolutional_63\n",
      "Parsing section convolutional_64\n",
      "Parsing section convolutional_65\n",
      "Parsing section convolutional_66\n",
      "Parsing section yolo_1\n",
      "Parsing section route_2\n",
      "Parsing section convolutional_67\n",
      "Parsing section upsample_1\n",
      "Parsing section route_3\n",
      "Parsing section convolutional_68\n",
      "Parsing section convolutional_69\n",
      "Parsing section convolutional_70\n",
      "Parsing section convolutional_71\n",
      "Parsing section convolutional_72\n",
      "Parsing section convolutional_73\n",
      "Parsing section convolutional_74\n",
      "Parsing section yolo_2\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "out_index = []\n",
    "for section in cfg_parser.sections():\n",
    "    print('Parsing section {}'.format(section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:19:00.817458Z",
     "start_time": "2019-02-14T07:19:00.767461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing section : net_0 ===\n",
      "=== Parsing section : convolutional_0 ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a0ca675090b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Darknet serializes convolutional weights as:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# [bias/beta, [gamma, mean, variance], conv_weights]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprev_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mweights_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_layer_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
     ]
    }
   ],
   "source": [
    "for section in cfg_parser.sections()[:5]:\n",
    "    print(\"=== Parsing section : {} ===\".format(section))\n",
    "    #===============\n",
    "    if section.startswith('convolutional'):\n",
    "        filters = int(cfg_parser[section]['filters'])\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        pad = int(cfg_parser[section]['pad'])\n",
    "        activation = cfg_parser[section]['activation']\n",
    "        batch_normalize = 'batch_normalize' in cfg_parser[section]\n",
    "\n",
    "        padding = 'same' if pad == 1 and stride == 1 else 'valid'\n",
    "\n",
    "        # Setting weights.\n",
    "        # Darknet serializes convolutional weights as:\n",
    "        # [bias/beta, [gamma, mean, variance], conv_weights]\n",
    "        prev_layer_shape = K.int_shape(prev_layer)\n",
    "\n",
    "        weights_shape = (size, size, prev_layer_shape[-1], filters)\n",
    "        darknet_w_shape = (filters, weights_shape[2], size, size)\n",
    "        weights_size = np.product(weights_shape)\n",
    "\n",
    "        print('conv2d', 'bn' if batch_normalize else '  ', activation, weights_shape)\n",
    "\n",
    "        conv_bias = np.ndarray(shape=(filters, ), dtype='float32',\n",
    "                               buffer=weights_file.read(filters * 4))\n",
    "        count += filters\n",
    "\n",
    "        if batch_normalize:\n",
    "            bn_weights = np.ndarray(shape=(3, filters), dtype='float32',\n",
    "                                    buffer=weights_file.read(filters * 12))\n",
    "            count += 3 * filters\n",
    "\n",
    "            bn_weight_list = [\n",
    "                bn_weights[0],  # scale gamma\n",
    "                conv_bias,      # shift beta\n",
    "                bn_weights[1],  # running mean\n",
    "                bn_weights[2],  # running var\n",
    "            ]\n",
    "\n",
    "        conv_weights = np.ndarray(shape=darknet_w_shape,\n",
    "                                  dtype='float32',\n",
    "                                  buffer=weights_file.read(weights_size * 4))\n",
    "        count += weights_size\n",
    "\n",
    "        # DarkNet conv_weights are serialized Caffe-style:\n",
    "        # (out_dim, in_dim, height, width)\n",
    "        # We would like to set these to Tensorflow order:\n",
    "        # (height, width, in_dim, out_dim)\n",
    "        conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "        conv_weights = [conv_weights] if batch_normalize else [\n",
    "            conv_weights, conv_bias\n",
    "        ]\n",
    "\n",
    "        # Create Conv2D layer\n",
    "        if stride>1:\n",
    "            # Darknet uses left and top padding instead of 'same' mode\n",
    "            prev_layer = ZeroPadding2D(((1,0),(1,0)))(prev_layer)\n",
    "        conv_layer = (Conv2D(\n",
    "            filters, (size, size),\n",
    "            strides=(stride, stride),\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            use_bias=not batch_normalize,\n",
    "            weights=conv_weights,\n",
    "            activation=act_fn,\n",
    "            padding=padding))(prev_layer)\n",
    "\n",
    "        if batch_normalize:\n",
    "            conv_layer = (BatchNormalization(\n",
    "                weights=bn_weight_list))(conv_layer)\n",
    "        prev_layer = conv_layer\n",
    "\n",
    "        if activation == 'linear':\n",
    "            all_layers.append(prev_layer)\n",
    "        elif activation == 'leaky':\n",
    "            act_layer = LeakyReLU(alpha=0.1)(prev_layer)\n",
    "            prev_layer = act_layer\n",
    "            all_layers.append(act_layer)\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('route'):\n",
    "        ids = [int(i) for i in cfg_parser[section]['layers'].split(',')]\n",
    "        layers = [all_layers[i] for i in ids]\n",
    "        if len(layers) > 1:\n",
    "            print('Concatenating route layers:', layers)\n",
    "            concatenate_layer = Concatenate()(layers)\n",
    "            all_layers.append(concatenate_layer)\n",
    "            prev_layer = concatenate_layer\n",
    "        else:\n",
    "            skip_layer = layers[0]  # only one layer to route\n",
    "            all_layers.append(skip_layer)\n",
    "            prev_layer = skip_layer\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('maxpool'):\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        all_layers.append(\n",
    "            MaxPooling2D(\n",
    "                pool_size=(size, size),\n",
    "                strides=(stride, stride),\n",
    "                padding='same')(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('shortcut'):\n",
    "        index = int(cfg_parser[section]['from'])\n",
    "        activation = cfg_parser[section]['activation']\n",
    "        assert activation == 'linear', 'Only linear activation supported.'\n",
    "        all_layers.append(Add()([all_layers[index], prev_layer]))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('upsample'):\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        assert stride == 2, 'Only stride=2 supported.'\n",
    "        all_layers.append(UpSampling2D(stride)(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('yolo'):\n",
    "        out_index.append(len(all_layers)-1)\n",
    "        all_layers.append(None)\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('net'):\n",
    "        pass\n",
    "\n",
    "    #===============\n",
    "    else:\n",
    "        raise ValueError('Unsupported section header type: {}'.format(section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

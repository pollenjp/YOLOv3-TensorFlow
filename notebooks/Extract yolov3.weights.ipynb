{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/qqwweee/keras-yolo3\n",
    "    - `convert.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Config\" data-toc-modified-id=\"Config-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Config</a></span><ul class=\"toc-item\"><li><span><a href=\"#Path\" data-toc-modified-id=\"Path-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Path</a></span></li></ul></li><li><span><a href=\"#yolov3.weight\" data-toc-modified-id=\"yolov3.weight-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>yolov3.weight</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:01.451682Z",
     "start_time": "2019-02-15T00:21:01.435781Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "import pathlib\n",
    "import collections\n",
    "import io\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:01.644498Z",
     "start_time": "2019-02-15T00:21:01.636975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:01.939150Z",
     "start_time": "2019-02-15T00:21:01.931116Z"
    }
   },
   "outputs": [],
   "source": [
    "HOME_Path = pathlib.Path(os.getcwd()).parents[0]\n",
    "\n",
    "#config_Path = HOME_Path / \"cfg\" / \"yolov3.org.cfg\"\n",
    "config_Path = HOME_Path / \"cfg\" / \"yolov3.pollenjp.cfg\"\n",
    "assert config_Path.exists()\n",
    "assert config_Path.name.endswith(\".cfg\"), \"{} is not a .cfg file\".format(config_Path)\n",
    "\n",
    "weights_Path = HOME_Path / \"weights\" / \"yolov3.weights\"\n",
    "assert weights_Path.exists()\n",
    "assert weights_Path.name.endswith(\".weights\"), \"{} is not a .weights file\".format(weights_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yolov3.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:02.233459Z",
     "start_time": "2019-02-15T00:21:02.229861Z"
    }
   },
   "outputs": [],
   "source": [
    "weights_file = open(file=str(weights_Path), mode=\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:02.337336Z",
     "start_time": "2019-02-15T00:21:02.331458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Header\n",
      "major    : 0\n",
      "minor    : 2\n",
      "revision : 0\n",
      "seen     : [32013312]\n"
     ]
    }
   ],
   "source": [
    "major, minor, revision = np.ndarray(shape=(3, ), dtype='int32', buffer=weights_file.read(12))\n",
    "\n",
    "if (major*10+minor)>=2 and major<1000 and minor<1000:\n",
    "    seen = np.ndarray(shape=(1,), dtype='int64', buffer=weights_file.read(8))\n",
    "else:\n",
    "    seen = np.ndarray(shape=(1,), dtype='int32', buffer=weights_file.read(4))\n",
    "\n",
    "print(\"Weights Header\\n\" +\n",
    "      \"major    : {}\\n\".format(major) +\n",
    "      \"minor    : {}\\n\".format(minor) +\n",
    "      \"revision : {}\\n\".format(revision) +\n",
    "      \"seen     : {}\".format(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:02.548844Z",
     "start_time": "2019-02-15T00:21:02.535974Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_cfg_file(config_filepath):\n",
    "    \"\"\"\n",
    "    Convert all config sections to have unique names.\n",
    "    |-Parameters\n",
    "    | config_filepath | str |\n",
    "    |-Return\n",
    "    | cfg_parser| configparser.ConfigParser |\n",
    "    |-Ref\n",
    "    | https://github.com/qqwweee/keras-yolo3/blob/e6598d13c703029b2686bc2eb8d5c09badf42992/convert.py\n",
    "    \"\"\"\n",
    "    import collections\n",
    "    import io\n",
    "    import configparser\n",
    "    # [defaultdict の 利用 - Python defaultdict の使い方 - Qiita](https://qiita.com/xza/items/72a1b07fcf64d1f4bdb7#defaultdict-%E3%81%AE-%E5%88%A9%E7%94%A8)\n",
    "    section_counters = collections.defaultdict(int)  # defaultdictは関数でdictのvalueを初期化\n",
    "\n",
    "    output_stream = io.StringIO()\n",
    "    with open(config_filepath) as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('['):\n",
    "                section = line.strip().strip('[]')\n",
    "                _section = section + '_' + str(section_counters[section])\n",
    "                section_counters[section] += 1\n",
    "                line = line.replace(section, _section)\n",
    "            output_stream.write(line)\n",
    "    output_stream.seek(0)\n",
    "\n",
    "    cfg_parser = configparser.ConfigParser()\n",
    "    cfg_parser.read_file(f=output_stream)\n",
    "    return cfg_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:02.872455Z",
     "start_time": "2019-02-15T00:21:02.838132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Darknet config.\n"
     ]
    }
   ],
   "source": [
    "print('Parsing Darknet config.')\n",
    "cfg_parser = read_cfg_file(config_filepath=str(config_Path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:03.248589Z",
     "start_time": "2019-02-15T00:21:03.237955Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_decay = float(cfg_parser['net_0']['decay']) if 'net_0' in cfg_parser.sections() else 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T00:21:03.636152Z",
     "start_time": "2019-02-15T00:21:03.562520Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing section :           net_0 ===\n",
      "    batch           :      64\n",
      "    subdivisions    :      16\n",
      "    width           :     608\n",
      "    height          :     608\n",
      "    channels        :       3\n",
      "    momentum        :     0.9\n",
      "    decay           :  0.0005\n",
      "    angle           :       0\n",
      "    saturation      :     1.5\n",
      "    exposure        :     1.5\n",
      "    hue             :      .1\n",
      "    learning_rate   :   0.001\n",
      "    burn_in         :    1000\n",
      "    max_batches     :  500200\n",
      "    policy          :   steps\n",
      "    steps           : 400000,450000\n",
      "    scales          :   .1,.1\n",
      "=== Parsing section : convolutional_0 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :      32\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_1 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :      64\n",
      "    size            :       3\n",
      "    stride          :       2\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_2 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :      32\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_3 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :      64\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_0 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_4 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       3\n",
      "    stride          :       2\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_5 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :      64\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_6 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_1 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_7 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :      64\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_8 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_2 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_9 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       2\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_10 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_11 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_3 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_12 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_13 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_4 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_14 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_15 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_5 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_16 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_17 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_6 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_18 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_19 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_7 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_20 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_21 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_8 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_22 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_23 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      shortcut_9 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_24 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_25 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_10 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_26 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       2\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_27 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_28 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_11 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_29 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_30 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_12 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_31 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_32 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_13 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_33 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_34 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_14 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_35 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_36 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_15 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_37 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_38 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_16 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_39 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_40 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_17 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_41 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_42 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_18 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_43 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :    1024\n",
      "    size            :       3\n",
      "    stride          :       2\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_44 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_45 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :    1024\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_19 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_46 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_47 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :    1024\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_20 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_48 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_49 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :    1024\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_21 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section : convolutional_50 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_51 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :    1024\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :     shortcut_22 ===\n",
      "    from            :      -3\n",
      "    activation      :  linear\n",
      "=== Parsing section :         scale_0 ===\n",
      "=== Parsing section : convolutional_52 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_53 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :    1024\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_54 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_55 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :    1024\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_56 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     512\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_57 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :    1024\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_58 ===\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     255\n",
      "    activation      :  linear\n",
      "=== Parsing section :          yolo_0 ===\n",
      "    mask            :   6,7,8\n",
      "    anchors         : 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
      "    classes         :      80\n",
      "    num             :       9\n",
      "    jitter          :      .3\n",
      "    ignore_thresh   :      .7\n",
      "    truth_thresh    :       1\n",
      "    random          :       1\n",
      "=== Parsing section :         route_0 ===\n",
      "    layers          :      -4\n",
      "=== Parsing section :         scale_1 ===\n",
      "=== Parsing section : convolutional_59 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      upsample_0 ===\n",
      "    stride          :       2\n",
      "=== Parsing section :         route_1 ===\n",
      "    layers          :  -1, 61\n",
      "=== Parsing section : convolutional_60 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_61 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     512\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_62 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_63 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     512\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_64 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     256\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_65 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     512\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_66 ===\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     255\n",
      "    activation      :  linear\n",
      "=== Parsing section :          yolo_1 ===\n",
      "    mask            :   3,4,5\n",
      "    anchors         : 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
      "    classes         :      80\n",
      "    num             :       9\n",
      "    jitter          :      .3\n",
      "    ignore_thresh   :      .7\n",
      "    truth_thresh    :       1\n",
      "    random          :       1\n",
      "=== Parsing section :         route_2 ===\n",
      "    layers          :      -4\n",
      "=== Parsing section :         scale_2 ===\n",
      "=== Parsing section : convolutional_67 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section :      upsample_1 ===\n",
      "    stride          :       2\n",
      "=== Parsing section :         route_3 ===\n",
      "    layers          :  -1, 36\n",
      "=== Parsing section : convolutional_68 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_69 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     256\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_70 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_71 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     256\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_72 ===\n",
      "    batch_normalize :       1\n",
      "    filters         :     128\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_73 ===\n",
      "    batch_normalize :       1\n",
      "    size            :       3\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     256\n",
      "    activation      :   leaky\n",
      "=== Parsing section : convolutional_74 ===\n",
      "    size            :       1\n",
      "    stride          :       1\n",
      "    pad             :       1\n",
      "    filters         :     255\n",
      "    activation      :  linear\n",
      "=== Parsing section :          yolo_2 ===\n",
      "    mask            :   0,1,2\n",
      "    anchors         : 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
      "    classes         :      80\n",
      "    num             :       9\n",
      "    jitter          :      .3\n",
      "    ignore_thresh   :      .7\n",
      "    truth_thresh    :       1\n",
      "    random          :       1\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "out_index = []\n",
    "for section in cfg_parser.sections():\n",
    "    print(\"=== Parsing section : {:>15} ===\".format(section))\n",
    "    for key, value in cfg_parser[section].items():\n",
    "        print(\"    {:<15} : {:>7}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T01:35:08.879569Z",
     "start_time": "2019-02-15T01:35:08.869362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "configparser.SectionProxy"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cfg_parser[\"net_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T01:40:05.918628Z",
     "start_time": "2019-02-15T01:40:05.908381Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def split_layer_structure(config_parser):\n",
    "    \"\"\"\n",
    "    |-Return\n",
    "    | structure_list | [features, scale_0, scale_1, scale_2]\n",
    "    |                |\n",
    "    |                | ex) [OrderedDict((section, configparser.SectionProxy),\n",
    "    |                |                  (section, configparser.SectionProxy),\n",
    "    |                |                  (section, configparser.SectionProxy)),\n",
    "    |                |      OrderedDict((section, configparser.SectionProxy),\n",
    "    |                |                  (section, configparser.SectionProxy),\n",
    "    |                |                  (section, configparser.SectionProxy)),\n",
    "    |                |      OrderedDict((section, configparser.SectionProxy)),\n",
    "    |                |      OrderedDict((section, configparser.SectionProxy))]\n",
    "    \"\"\"\n",
    "    import collections\n",
    "    structure_list = [collections.OrderedDict()]\n",
    "    idx = 0\n",
    "    for section in cfg_parser.sections():\n",
    "        if section == \"scale_0\" or section == \"scale_1\" or section == \"scale_2\":\n",
    "            structure_list.append(collections.OrderedDict())\n",
    "            idx += 1\n",
    "            continue\n",
    "        structure_list[idx][section] = cfg_parser[section]\n",
    "    return structure_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T01:41:13.068444Z",
     "start_time": "2019-02-15T01:41:13.061313Z"
    }
   },
   "outputs": [],
   "source": [
    "features_odict, scale_0_odict, scale_1_odict, scale_2_odict = split_layer_structure(config_parser=cfg_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:19:00.817458Z",
     "start_time": "2019-02-14T07:19:00.767461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing section : net_0 ===\n",
      "=== Parsing section : convolutional_0 ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a0ca675090b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Darknet serializes convolutional weights as:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# [bias/beta, [gamma, mean, variance], conv_weights]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprev_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mweights_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_layer_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
     ]
    }
   ],
   "source": [
    "weights_dict = {}\n",
    "\n",
    "for section in cfg_parser.sections()[:5]:\n",
    "    print(\"=== Parsing section : {} ===\".format(section))\n",
    "    #===============\n",
    "    if section.startswith('convolutional'):\n",
    "\n",
    "        weights_dict\n",
    "\n",
    "        filters = int(cfg_parser[section]['filters'])\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        pad = int(cfg_parser[section]['pad'])\n",
    "        activation = cfg_parser[section]['activation']\n",
    "        batch_normalize = 'batch_normalize' in cfg_parser[section]\n",
    "\n",
    "        padding = 'same' if pad == 1 and stride == 1 else 'valid'\n",
    "\n",
    "        # Setting weights.\n",
    "        # Darknet serializes convolutional weights as:\n",
    "        # [bias/beta, [gamma, mean, variance], conv_weights]\n",
    "        prev_layer_shape = K.int_shape(prev_layer)\n",
    "\n",
    "        weights_shape = (size, size, prev_layer_shape[-1], filters)\n",
    "        darknet_w_shape = (filters, weights_shape[2], size, size)\n",
    "        weights_size = np.product(weights_shape)\n",
    "\n",
    "        print('conv2d', 'bn' if batch_normalize else '  ', activation, weights_shape)\n",
    "\n",
    "        conv_bias = np.ndarray(shape=(filters, ), dtype='float32',\n",
    "                               buffer=weights_file.read(filters * 4))\n",
    "        count += filters\n",
    "\n",
    "        if batch_normalize:\n",
    "            bn_weights = np.ndarray(shape=(3, filters), dtype='float32',\n",
    "                                    buffer=weights_file.read(filters * 12))\n",
    "            count += 3 * filters\n",
    "\n",
    "            bn_weight_list = [\n",
    "                bn_weights[0],  # scale gamma\n",
    "                conv_bias,      # shift beta\n",
    "                bn_weights[1],  # running mean\n",
    "                bn_weights[2],  # running var\n",
    "            ]\n",
    "\n",
    "        conv_weights = np.ndarray(shape=darknet_w_shape,\n",
    "                                  dtype='float32',\n",
    "                                  buffer=weights_file.read(weights_size * 4))\n",
    "        count += weights_size\n",
    "\n",
    "        # DarkNet conv_weights are serialized Caffe-style:\n",
    "        # (out_dim, in_dim, height, width)\n",
    "        # We would like to set these to Tensorflow order:\n",
    "        # (height, width, in_dim, out_dim)\n",
    "        conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "        conv_weights = [conv_weights] if batch_normalize else [\n",
    "            conv_weights, conv_bias\n",
    "        ]\n",
    "\n",
    "        # Create Conv2D layer\n",
    "        if stride>1:\n",
    "            # Darknet uses left and top padding instead of 'same' mode\n",
    "            prev_layer = ZeroPadding2D(((1,0),(1,0)))(prev_layer)\n",
    "        conv_layer = (Conv2D(\n",
    "            filters, (size, size),\n",
    "            strides=(stride, stride),\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            use_bias=not batch_normalize,\n",
    "            weights=conv_weights,\n",
    "            activation=act_fn,\n",
    "            padding=padding))(prev_layer)\n",
    "\n",
    "        if batch_normalize:\n",
    "            conv_layer = (BatchNormalization(\n",
    "                weights=bn_weight_list))(conv_layer)\n",
    "        prev_layer = conv_layer\n",
    "\n",
    "        if activation == 'linear':\n",
    "            all_layers.append(prev_layer)\n",
    "        elif activation == 'leaky':\n",
    "            act_layer = LeakyReLU(alpha=0.1)(prev_layer)\n",
    "            prev_layer = act_layer\n",
    "            all_layers.append(act_layer)\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('route'):\n",
    "        ids = [int(i) for i in cfg_parser[section]['layers'].split(',')]\n",
    "        layers = [all_layers[i] for i in ids]\n",
    "        if len(layers) > 1:\n",
    "            print('Concatenating route layers:', layers)\n",
    "            concatenate_layer = Concatenate()(layers)\n",
    "            all_layers.append(concatenate_layer)\n",
    "            prev_layer = concatenate_layer\n",
    "        else:\n",
    "            skip_layer = layers[0]  # only one layer to route\n",
    "            all_layers.append(skip_layer)\n",
    "            prev_layer = skip_layer\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('maxpool'):\n",
    "        size = int(cfg_parser[section]['size'])\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        all_layers.append(\n",
    "            MaxPooling2D(\n",
    "                pool_size=(size, size),\n",
    "                strides=(stride, stride),\n",
    "                padding='same')(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('shortcut'):\n",
    "        index = int(cfg_parser[section]['from'])\n",
    "        activation = cfg_parser[section]['activation']\n",
    "        assert activation == 'linear', 'Only linear activation supported.'\n",
    "        all_layers.append(Add()([all_layers[index], prev_layer]))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('upsample'):\n",
    "        stride = int(cfg_parser[section]['stride'])\n",
    "        assert stride == 2, 'Only stride=2 supported.'\n",
    "        all_layers.append(UpSampling2D(stride)(prev_layer))\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('yolo'):\n",
    "        out_index.append(len(all_layers)-1)\n",
    "        all_layers.append(None)\n",
    "        prev_layer = all_layers[-1]\n",
    "\n",
    "    #===============\n",
    "    elif section.startswith('net'):\n",
    "        pass\n",
    "\n",
    "    #===============\n",
    "    else:\n",
    "        raise ValueError('Unsupported section header type: {}'.format(section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "200"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
